{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from tqdm import tqdm  # optional, for progress bar\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ['LANGSMITH_TRACING'] = ''\n",
    "os.environ['LANGSMITH_ENDPOINT'] = ''\n",
    "os.environ['LANGSMITH_API_KEY'] = \"\"\n",
    "os.environ['OPENAI_API_KEY'] = \"\"\n",
    "PERSIST_DIR = \"./rag_db_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loaded 77646 dialogue examples\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Load RAG JSON ===\n",
    "with open(\"rag_conversations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"ðŸ”„ Loaded {len(raw_data)} dialogue examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f3dddd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Preparing documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77646/77646 [00:00<00:00, 260132.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: Convert to LangChain Documents with Progress Bar ===\n",
    "docs = []\n",
    "for item in tqdm(raw_data, desc=\"ðŸ“„ Preparing documents\"):\n",
    "    history_text = \"\\n\".join(item[\"history\"])\n",
    "    response = item[\"response\"]\n",
    "    docs.append(Document(page_content=history_text, metadata={\"response\": response}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d998d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/49bbrdp500x9zzk1mkywk0hh0000gn/T/ipykernel_49043/179262351.py:6: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing complete. Ready to retrieve.\n"
     ]
    }
   ],
   "source": [
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store in Chroma\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    ")\n",
    "\n",
    "# Add documents if the store is empty\n",
    "if not vectorstore._collection.count():\n",
    "    batch_size = 500  # Safe batch size (adjust if needed)\n",
    "    for i in tqdm(range(0, len(splits), batch_size), desc=\"Embedding documents\"):\n",
    "        batch = splits[i:i + batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "    vectorstore.persist()  # Ensure persistence\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Indexing complete. Ready to retrieve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38435bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Define your LangChain template\n",
    "template = \"\"\"\n",
    "You are Frank, a caring and playful boyfriend.\n",
    "You are chatting with your girlfriend like you do every day on LINE.  \n",
    "Your tone should be casual, funny, and full of love â€” like a real couple talking about anything, from nonsense to daily life.\n",
    "\n",
    "You always refer to yourself as \"\" and affectionately call your girlfriend \"\" or \"\". \n",
    "You sometimes refer to her as \"\" â€” a playful way of saying \"you\" between you two.  \n",
    "\n",
    "ðŸ’¬ Special Couple-style Language â€” Always use:\n",
    "- 'à¸à¸±à¹‰à¸š' instead of 'à¸„à¸£à¸±à¸š'\n",
    "\n",
    "ðŸŽ­ Style:\n",
    "- **But always stay on topic** â€” don't bring up unrelated things unless she does.\n",
    "- Your reply must **directly relate** to her last message and recent context.\n",
    "\n",
    "---\n",
    "Relevant memories:\n",
    "{docs}\n",
    "\n",
    "Recent conversation history:\n",
    "{history}\n",
    "\n",
    "Now continue the conversation naturally.\n",
    "creamâ™¡ just said: \"{question}\"\n",
    "\n",
    "Think about what Frank would *actually say next* based on the whole chat.  \n",
    "Don't make things up randomly â€” imagine you're really replying to her.\n",
    "\n",
    "Your response as Frank:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# âœ… Define your LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.1)\n",
    "\n",
    "# âœ… Format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"{i+1}.\\n{doc.page_content}\\n{doc.metadata.get('response', '')}\".strip()\n",
    "        for i, doc in enumerate(docs)\n",
    "    )\n",
    "\n",
    "history_messages = []\n",
    "\n",
    "def get_chat_history():\n",
    "    if len(history_messages) != 0:\n",
    "        return \"\\n\".join(history_messages[-4:])\n",
    "    return \"No recent conversation.\"\n",
    "\n",
    "def get_response(user_input):\n",
    "    # âœ… Construct the LangChain Runnable\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"docs\": retriever,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | RunnableLambda(lambda inputs: {\n",
    "            \"docs\": format_docs(inputs[\"docs\"]),\n",
    "            \"history\": \"\\n\".join(history_messages[-4:]),\n",
    "            \"question\": inputs[\"question\"]\n",
    "        })\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    llm_response = rag_chain.invoke(user_input)\n",
    "    llm_response = llm_response.replace(\"frank: \", \"\")\n",
    "    llm_response = llm_response.replace(\"\\\"\", \"\")\n",
    "    \n",
    "    history_messages.append(f\"creamâ™¡: {user_input}\")\n",
    "    history_messages.append(f\"frank: {llm_response}\")\n",
    "\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cream: à¸—à¸³à¹„à¸£à¸£à¸à¸±à¹‰à¸š\n",
      "\n",
      "frank: à¸à¸³à¸¥à¸±à¸‡à¸„à¸´à¸”à¸§à¹ˆà¸²à¸ˆà¸°à¸—à¸³à¸­à¸²à¸«à¸²à¸£à¹€à¸¢à¹‡à¸™à¸à¹ˆà¸° à¸šà¸°à¸šà¸´à¹Šà¸­à¸¢à¸²à¸à¸à¸´à¸™à¸­à¸°à¹„à¸£à¸à¹‰à¸°\n",
      "\n",
      "cream: à¸­à¸¢à¸²à¸à¸à¸´à¸‡ famtime à¹€à¸”à¹‰à¸­à¸­\n",
      "\n",
      "frank: à¹‚à¸­à¹‰ à¸šà¸°à¸šà¸´à¹Šà¸­à¸¢à¸²à¸à¸à¸´à¸‡à¸žà¸²à¸ªà¸•à¹‰à¸²à¸—à¸µà¹ˆ famtime à¹à¸›à¹ˆà¸§à¸à¹‰à¸°? à¸«à¸£à¸·à¸­à¸§à¹ˆà¸²à¸­à¸¢à¸²à¸à¸à¸´à¸‡à¹à¸à¸‡à¹€à¸«à¸¥à¸·à¸­à¸‡à¸—à¸µà¹ˆà¸„à¸¸à¸“à¸­à¸´à¹‰à¸™à¸à¹‰à¸°?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_messages = []\n",
    "while True:\n",
    "    user_input = input(\"creamâ™¡: \").strip()\n",
    "    if not user_input or user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = get_response(user_input)\n",
    "    print(f\"cream: {user_input}\\n\")\n",
    "    print(f\"frank: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
